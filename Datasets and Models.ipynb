{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOPG6MlQ8LVqIWqb9R0y0Zw"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard","accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# **Tensorflow Datasets and Keras Models**\n","\n","The aim of this notebook is to do several tests with some Tensorflow datasets and Keras models, that are going to be detailed hereunder.\n","The prerequisites for these tests are installing Tensorflow datasets and Keras, as well as all the modules that are considered necessary for the different sections."],"metadata":{"id":"dxomka38fKWo"}},{"cell_type":"code","source":["# TensorFlow and tf.keras\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense\n","\n","# Commonly used modules\n","import numpy as np\n","import os\n","import sys\n","\n","# Images, plots, display, and visualization\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","import seaborn as sns\n","import cv2\n","import imutils\n","import IPython\n","from six.moves import urllib\n","\n","!pip install scikit-image\n","import skimage\n","from skimage import metrics\n","\n","#Download tensorflow datasets\n","!pip install tensorflow-datasets\n","import tensorflow_datasets as tfds"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SPxvIxvpCkvy","executionInfo":{"status":"ok","timestamp":1674642770013,"user_tz":-60,"elapsed":21427,"user":{"displayName":"NÃºria Camps","userId":"06463818817438967590"}},"outputId":"a436a38b-2eab-42b7-8ca8-3386bb57f689"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: scikit-image in /usr/local/lib/python3.8/dist-packages (0.18.3)\n","Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.8/dist-packages (from scikit-image) (2022.10.10)\n","Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.8/dist-packages (from scikit-image) (2.9.0)\n","Requirement already satisfied: numpy>=1.16.5 in /usr/local/lib/python3.8/dist-packages (from scikit-image) (1.21.6)\n","Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.8/dist-packages (from scikit-image) (3.0)\n","Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-image) (3.2.2)\n","Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from scikit-image) (1.4.1)\n","Requirement already satisfied: pillow!=7.1.0,!=7.1.1,>=4.3.0 in /usr/local/lib/python3.8/dist-packages (from scikit-image) (7.1.2)\n","Requirement already satisfied: scipy>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from scikit-image) (1.7.3)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image) (0.11.0)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image) (3.0.9)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image) (1.4.4)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.1->matplotlib!=3.0.0,>=2.0.0->scikit-image) (1.15.0)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.8/dist-packages (4.8.1)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from tensorflow-datasets) (4.64.1)\n","Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.8/dist-packages (from tensorflow-datasets) (1.12.0)\n","Requirement already satisfied: promise in /usr/local/lib/python3.8/dist-packages (from tensorflow-datasets) (2.3)\n","Requirement already satisfied: etils[enp,epath]>=0.9.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow-datasets) (1.0.0)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.8/dist-packages (from tensorflow-datasets) (5.4.8)\n","Requirement already satisfied: dill in /usr/local/lib/python3.8/dist-packages (from tensorflow-datasets) (0.3.6)\n","Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from tensorflow-datasets) (7.1.2)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow-datasets) (2.25.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from tensorflow-datasets) (1.15.0)\n","Requirement already satisfied: toml in /usr/local/lib/python3.8/dist-packages (from tensorflow-datasets) (0.10.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from tensorflow-datasets) (1.21.6)\n","Requirement already satisfied: importlib-resources in /usr/local/lib/python3.8/dist-packages (from tensorflow-datasets) (5.10.2)\n","Requirement already satisfied: protobuf>=3.12.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow-datasets) (3.19.6)\n","Requirement already satisfied: dm-tree in /usr/local/lib/python3.8/dist-packages (from tensorflow-datasets) (0.1.8)\n","Requirement already satisfied: absl-py in /usr/local/lib/python3.8/dist-packages (from tensorflow-datasets) (1.3.0)\n","Requirement already satisfied: termcolor in /usr/local/lib/python3.8/dist-packages (from tensorflow-datasets) (2.2.0)\n","Requirement already satisfied: zipp in /usr/local/lib/python3.8/dist-packages (from etils[enp,epath]>=0.9.0->tensorflow-datasets) (3.11.0)\n","Requirement already satisfied: typing_extensions in /usr/local/lib/python3.8/dist-packages (from etils[enp,epath]>=0.9.0->tensorflow-datasets) (4.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->tensorflow-datasets) (2022.12.7)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->tensorflow-datasets) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->tensorflow-datasets) (2.10)\n","Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->tensorflow-datasets) (4.0.0)\n","Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow-metadata->tensorflow-datasets) (1.58.0)\n"]}]},{"cell_type":"markdown","source":["## Switch function implementation\n","Hereunder, the implementation of a typical switch function is proposed. It is going to be useful later on to choose among the available Keras models to be generated."],"metadata":{"id":"dP7vCYS6jthM"}},{"cell_type":"code","source":["class switch:\n","\n","\tdef __init__(self, variable, comparator=None, strict=False):\n","\t\tself.variable = variable\n","\t\tself.matched = False\n","\t\tself.matching = False\n","\t\tif comparator:\n","\t\t\tself.comparator = comparator\n","\t\telse:\n","\t\t\tself.comparator = lambda x, y: x == y\n","\t\tself.strict = strict\n","\n","\tdef __enter__(self):\n","\t\treturn self\n","\n","\tdef __exit__(self, exc_type, exc_val, exc_tb):\n","\t\tpass\n","\n","\tdef case(self, expr, break_=False):\n","\t\tif self.strict:\n","\t\t\tif self.matched:\n","\t\t\t\treturn False\n","\t\tif self.matching or self.comparator(self.variable, expr):\n","\t\t\tif not break_:\n","\t\t\t\tself.matching = True\n","\t\t\telse:\n","\t\t\t\tself.matched = True\n","\t\t\t\tself.matching = False\n","\t\t\treturn True\n","\t\telse:\n","\t\t\treturn False\n","\n","\tdef default(self):\n","\t\treturn not self.matched and not self.matching"],"metadata":{"id":"qCInW6EYjt37"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Preprocessing block\n","In this code block, a set of functions to preprocess the data of the different datasets are implemented.\n","For that, a batch size of 32 examples is defined.\n","\n","The *configure_for_performance* function, prepares the data of the chosen dataset to be trained.\n","\n","For that, first it calls the function *input_preprocess* to appply one-hot encoding to the train and test labels.\n","Then, it caches the dataset in order to save some operations, such as data reading or file opening, from being executed during each epoch.\n","\n","It also shuffles the data, allocating a buffer size of 1000 to pick random entries and by means of the *Dataset.batch* function, stacks 32 consecutive elements of the dataset into a single element.\n","\n","Finally, it applies prefetching to overlap the preprocessing and model execution of training steps. In this way, while the model is executing training step n, the input pipeline will be reading the data for step n+1, thus reducing the step time of the training and the time to extract the data.\n","\n","The *resize_images* function uses lambda transformations to resize the images of the used dataset so that they have a shape that matches the input shape of the chosen model."],"metadata":{"id":"rIimHMP9oa-y"}},{"cell_type":"code","source":["#Preprocessing functions block\n","from tensorflow.python.data.ops.dataset_ops import AUTOTUNE\n","os.system(\"cls\")\n","\n","batch_size = 32\n","\n","def configure_for_performance(ds):\n","  ds=ds.map(input_preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n","  ds=ds.cache()  #cache dataset\n","  ds=ds.shuffle(buffer_size=1000)\n","  ds=ds.batch(batch_size=batch_size)\n","  ds=ds.prefetch(buffer_size=AUTOTUNE)\n","  return (ds)\n","\n","\n","def input_preprocess(image, label):\n","    label = tf.one_hot(tf.cast(label,tf.int64), NUM_CLASSES)\n","    return image, label\n","\n","\n","def resize_imgs(IMG_SIZE, ds_train, ds_test, ds_val):\n","  size = (IMG_SIZE, IMG_SIZE)\n","  #Resize images to input shape of used model\n","  ds_train = ds_train.map(lambda img, lbl:\n","                          (tf.image.resize(img, size, method='nearest'), lbl))\n","  ds_test = ds_test.map(lambda img, lbl:\n","                        (tf.image.resize(img, size, method='nearest'), lbl))\n","  if ds_val != None:\n","    ds_val= ds_val.map(lambda img, lbl:\n","                       (tf.image.resize(img, size), lbl))\n","    return ds_train, ds_test, ds_val\n","  else:\n","    return ds_train, ds_test"],"metadata":{"id":"B8te28iTUqhi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Load available datasets\n","The following functions (*load_mnist*, *load_fashion_mnist*, *load_celeba* and *load_cifar10*) are used to load the chosen dataset in each case.\n","All of them have a similar structure, in which first the data is loaded from the TensorFlow datasets and splitted into train (validation) and test sets and then, the number of classes of each dataset is obtained by means of their corresponding metadata.\n","These functions return the train and test datasets, as well as the number of classes corresponding to the chosen dataset.\n","\n","More information about the different datasets that have been used can be found in the following links:\n","\n","*   MNIST: https://www.tensorflow.org/datasets/catalog/mnist\n","*   Fashion-MNIST: https://www.tensorflow.org/datasets/catalog/fashion_mnist\n","*   CelebA: https://www.tensorflow.org/datasets/catalog/celeb_a\n","*   CIFAR-10: https://www.tensorflow.org/datasets/catalog/cifar10"],"metadata":{"id":"LUv0UZLheUJ8"}},{"cell_type":"code","source":["#Functions for loading different TensorFlow datasets\n","def load_mnist():\n","  \"\"\"This function loads MNIST dataset\"\"\"\n","  (ds_train, ds_test), ds_info = tfds.load(\n","      'mnist',\n","      split = [\"train\", \"test\"],\n","      with_info = True,\n","      as_supervised = True\n","      )\n","  num_classes = ds_info.features[\"label\"].num_classes\n","  return(ds_train, ds_test, num_classes)\n","\n","\n","def load_fashion_mnist():\n","  \"\"\"This function loads Fashion MNIST dataset\"\"\"\n","  (ds_train, ds_test), ds_info = tfds.load(\n","      'fashion_mnist',\n","      split = ['train', 'test'],\n","      with_info = True,\n","      as_supervised = True\n","      )\n","  num_classes = ds_info.features[\"label\"].num_classes\n","  return(ds_train, ds_test, num_classes)\n","\n","\n","def load_celeba():\n","  \"\"\"This function loads Celeb_a dataset\"\"\"\n","  (ds_train, ds_val, ds_test), ds_info = tfds.load(\n","      'celeb_a',\n","      split = ['train', 'validation', 'test'],\n","      with_info = True,\n","      as_supervised = True\n","      )\n","  num_classes = ds_info.features[\"label\"].num_classes\n","  return(ds_train, ds_test, ds_val, num_classes)\n","\n","\n","def load_cifar10():\n","  \"\"\"This function loads Cifar10 dataset\"\"\"\n","  (ds_train, ds_test), ds_info= tfds.load(\n","      'cifar10',\n","      split = ['train', 'test'],\n","      with_info = True,\n","      as_supervised = True\n","      )\n","  num_classes = ds_info.features[\"label\"].num_classes\n","  return(ds_train, ds_test, num_classes)"],"metadata":{"id":"orfdYlMpSljK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Generate available models\n","The function included hereunder (*get_model*) is used to get the chosen model among the imported Keras models.\n","\n","The choice is performed by means of the previously included switch function, that selects the model to generate depending on the model name received at its input.\n","\n","The function returns the generated model, which is already pre-trained.\n","\n","More information about the different pre-trained models (https://keras.io/api/applications/) that have been imported can be found in the following links:\n","*   Resnet50: https://keras.io/api/applications/resnet/#resnet50-function\n","*   MobileNet: https://keras.io/api/applications/mobilenet/\n","*   DenseNet121: https://keras.io/api/applications/densenet/#densenet121-function\n","*   EfficientNetB0: https://keras.io/api/applications/efficientnet/#efficientnetb0-function"],"metadata":{"id":"4PaAYhfYeX0T"}},{"cell_type":"code","source":["from tensorflow.keras.applications.resnet50 import ResNet50\n","from tensorflow.keras.preprocessing import image\n","from tensorflow.keras.applications.resnet50 import preprocess_input as resnet_preprocess_input, decode_predictions\n","\n","from tensorflow.keras.applications.mobilenet import MobileNet\n","from tensorflow.keras.applications.mobilenet import preprocess_input as mobilenet_preprocess_input, decode_predictions\n","\n","from tensorflow.keras.applications.densenet import DenseNet121\n","from tensorflow.keras.applications.densenet import preprocess_input as densenet_preprocess_input, decode_predictions\n","\n","from tensorflow.keras.applications.efficientnet import EfficientNetB0\n","from tensorflow.keras.applications.efficientnet import preprocess_input as efficientnet_preprocess_input, decode_predictions\n","\n","#Function to get the desired model\n","def get_model(model_name, classes):\n","  '''This function builds the desired model'''\n","  with switch(model_name) as m:\n","    if m.case('resnet', True): model=ResNet50(weights=None, classes=classes)\n","    if m.case('mobilenet', True): model=MobileNet(weights=None, classes=classes)\n","    if m.case('densenet', True): model=DenseNet121(weights=None, classes=classes)\n","    if m.case('efficientnet', True): model=EfficientNetB0(weights=None, classes=classes)\n","    if m.default(): print('error')\n","  return model"],"metadata":{"id":"mIa_fq7peaX7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Proposed pipeline\n","The aim of the following code block is to check the correct implementation of the functions explained above.\n","\n","In it, first of all, the desired dataset is loaded using one of the functions prepared for this purpose.\n","\n","Then, we take one example and check the image shape and its assigned label.\n","\n","The next step is to resize images so that their shape match the input shape of the models. In this case, input shape of all models is 224 by 224.\n","To continue, the train and test datasets are preprocessed with the *configure_for_perfomance* function.\n","\n","Once the datasets are preprocessed, the model is obtained using the *get_model* function, compiled and finally trained with the preprocessed train dataset to check the correct operation of the code developed."],"metadata":{"id":"gbW3xAHKyMHu"}},{"cell_type":"code","source":["#Load the desired dataset\n","ds_train, ds_test, NUM_CLASSES = load_celeba()"],"metadata":{"id":"sTeuWG6On8oS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Take an example\n","for image, label in ds_train.take(1):\n","  print(\"Image shape: \", image.numpy().shape)\n","  print(\"Label: \", label.numpy())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UluHuVZyn6Vd","executionInfo":{"status":"ok","timestamp":1673363396767,"user_tz":-60,"elapsed":335,"user":{"displayName":"NÃºria Camps","userId":"06463818817438967590"}},"outputId":"bd6619bf-12c6-41c0-d7a6-5aa42ae5a2ff"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Image shape:  (28, 28, 3)\n","Label:  29\n"]}]},{"cell_type":"code","source":["#Resize images to (224, 224)\n","IMG_SIZE = 224\n","ds_train, ds_test = resize_imgs(IMG_SIZE, ds_train, ds_test, ds_val=None)"],"metadata":{"id":"1xSTQRY4yh4f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Preprocess datasets\n","ds_train = configure_for_performance(ds_train)\n","ds_test = configure_for_performance(ds_test)"],"metadata":{"id":"EZqiSr1uuar0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Compile desired model\n","model = get_model('resnet', NUM_CLASSES)\n","model.compile(loss='categorical_crossentropy', optimizer=keras.optimizers.Adam())"],"metadata":{"id":"lIWWE1MZyhTu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Train model\n","model.fit(ds_train) "],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"smFA72j4MirZ","executionInfo":{"status":"ok","timestamp":1673363430738,"user_tz":-60,"elapsed":20973,"user":{"displayName":"NÃºria Camps","userId":"06463818817438967590"}},"outputId":"b146d40f-c1bd-4352-f9a0-931c1f048052"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["4/4 [==============================] - 21s 984ms/step - loss: 6.6013\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7fcf6c46bf70>"]},"metadata":{},"execution_count":20}]}]}