{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1dCgQmKpXBIaiZdki5nHON-TnC5TzrvII","timestamp":1674119781336}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **Microsoft Azure Cognitive Services study**\n","\n","The aim of this notebook is to carry out an study on the impact that beauty filters might have on ITAs, more specifically, on the one developed by Microsoft Azure.\n","\n","The prerequisites for this study are:\n","- Installing the Computer Vision Software Development Kit\n","- Installing the Python Imaging Library (PIL)\n","- Creating a folder called \"images\" in the same route as this notebook and add some images to this folder. In this case, the loaded images consist of the faces of people of different races, before and after a beauty filter has been applied."],"metadata":{"id":"po6xe5quONdP"}},{"cell_type":"markdown","source":["## **Prerequisites and imports**"],"metadata":{"id":"gYQ3aQzOZUrH"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0FjZq8Fj-PQY","executionInfo":{"status":"ok","timestamp":1674546451428,"user_tz":-60,"elapsed":11833,"user":{"displayName":"Núria Camps","userId":"06463818817438967590"}},"outputId":"74741074-72ae-4b3c-c149-0c7f03852d1b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting azure-cognitiveservices-vision-computervision\n","  Downloading azure_cognitiveservices_vision_computervision-0.9.0-py2.py3-none-any.whl (39 kB)\n","Collecting azure-common~=1.1\n","  Downloading azure_common-1.1.28-py2.py3-none-any.whl (14 kB)\n","Collecting msrest>=0.5.0\n","  Downloading msrest-0.7.1-py3-none-any.whl (85 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.4/85.4 KB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from msrest>=0.5.0->azure-cognitiveservices-vision-computervision) (2022.12.7)\n","Requirement already satisfied: requests-oauthlib>=0.5.0 in /usr/local/lib/python3.8/dist-packages (from msrest>=0.5.0->azure-cognitiveservices-vision-computervision) (1.3.1)\n","Collecting isodate>=0.6.0\n","  Downloading isodate-0.6.1-py2.py3-none-any.whl (41 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 KB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: requests~=2.16 in /usr/local/lib/python3.8/dist-packages (from msrest>=0.5.0->azure-cognitiveservices-vision-computervision) (2.25.1)\n","Collecting azure-core>=1.24.0\n","  Downloading azure_core-1.26.2-py3-none-any.whl (173 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.8/173.8 KB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: typing-extensions>=4.0.1 in /usr/local/lib/python3.8/dist-packages (from azure-core>=1.24.0->msrest>=0.5.0->azure-cognitiveservices-vision-computervision) (4.4.0)\n","Requirement already satisfied: six>=1.11.0 in /usr/local/lib/python3.8/dist-packages (from azure-core>=1.24.0->msrest>=0.5.0->azure-cognitiveservices-vision-computervision) (1.15.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests~=2.16->msrest>=0.5.0->azure-cognitiveservices-vision-computervision) (2.10)\n","Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests~=2.16->msrest>=0.5.0->azure-cognitiveservices-vision-computervision) (4.0.0)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests~=2.16->msrest>=0.5.0->azure-cognitiveservices-vision-computervision) (1.24.3)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.5.0->msrest>=0.5.0->azure-cognitiveservices-vision-computervision) (3.2.2)\n","Installing collected packages: azure-common, isodate, azure-core, msrest, azure-cognitiveservices-vision-computervision\n","Successfully installed azure-cognitiveservices-vision-computervision-0.9.0 azure-common-1.1.28 azure-core-1.26.2 isodate-0.6.1 msrest-0.7.1\n"]}],"source":["#Install the Computer Vision SDK\n","!pip install --upgrade azure-cognitiveservices-vision-computervision"]},{"cell_type":"code","source":["#Install the Python Imaging Library\n","!pip install pillow"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Uc4er5XH-ScS","executionInfo":{"status":"ok","timestamp":1674546466532,"user_tz":-60,"elapsed":8078,"user":{"displayName":"Núria Camps","userId":"06463818817438967590"}},"outputId":"1789a50a-72e7-4b4a-db7e-3ce8075c9525"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: pillow in /usr/local/lib/python3.8/dist-packages (7.1.2)\n"]}]},{"cell_type":"code","source":["from azure.cognitiveservices.vision.computervision import ComputerVisionClient\n","from azure.cognitiveservices.vision.computervision.models import OperationStatusCodes\n","from azure.cognitiveservices.vision.computervision.models import VisualFeatureTypes, Details\n","from msrest.authentication import CognitiveServicesCredentials\n","\n","\n","from array import array\n","import os\n","from PIL import Image\n","import sys\n","import time\n","import numpy as np\n","import json\n","import pandas as pd\n","from pandas import json_normalize"],"metadata":{"id":"BSpW3wHl-U8Z"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Authentication**\n","Use credentials to authenticate and create a client.\n","To do so, enter own key from Azure subscription."],"metadata":{"id":"hPIjB4weU1sJ"}},{"cell_type":"code","source":["'''\n","Authenticate\n","Authenticates your credentials and creates a client.\n","'''\n","subscription_key = \"enter_subscription_key_here\"\n","endpoint = \"https://imagetagging2023.cognitiveservices.azure.com/\"\n","\n","computervision_client = ComputerVisionClient(endpoint, CognitiveServicesCredentials(subscription_key))\n","'''\n","END - Authenticate\n","'''"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"Bv6u8Ao-sYuJ","executionInfo":{"status":"ok","timestamp":1674546704755,"user_tz":-60,"elapsed":239,"user":{"displayName":"Núria Camps","userId":"06463818817438967590"}},"outputId":"0df75e5c-dacd-4e35-df72-70232d86ff8a"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\nEND - Authenticate\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":9}]},{"cell_type":"markdown","source":["## **Load local images**\n","To carry out the study, load all the selected images from the races folders, separated by race and into original and beautified ones.\n","\n","The names of the race folders are the same for both the beautified and the original cases."],"metadata":{"id":"JV1ka1KRUzSC"}},{"cell_type":"code","source":["#Filename of the current notebook\n","__file__ = 'azure_tagging.ipynb'\n","\n","#Route of the beautified images\n","filter_beauty = \"images/beauty\"\n","beauty_folder = os.path.join(os.path.dirname(os.path.abspath(__file__)), filter_beauty)\n","\n","#Route of the original images\n","filter_original = \"images/original\"\n","original_folder = os.path.join(os.path.dirname(os.path.abspath(__file__)),filter_original)\n","\n","#Folder names for the different races\n","race_folders = os.listdir(beauty_folder)"],"metadata":{"id":"OF6kflGKfa2Y"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Tag an Image**\n","The aim of this section is to **obtain and store** a **set of tags** for each **loaded image**, depending on wheter it is beautified or not.\n","\n","For every race, the corresponding images are loaded one by one and the API is called to obtain the set of tags that are assigned to each of them. The sets of tags are stored in a list called tag_results, in the same order in which they have been obtained.\n","\n","An important point here is that no more than 20 calls can be generated to the API per minute. To handle this, the number of succesive calls that have been done in the last minute are controlled and, in case they reach 20, we wait one minute until we start calling the API again.\n","\n","This process is repeated for both the folder with the original images and the one with the beautified images."],"metadata":{"id":"fhhIKKhFddEs"}},{"cell_type":"code","source":["'''\n","Tag an Image - local\n","This example returns a tag (key word) for each thing in the image.\n","'''\n","tag_results = [ ]\n","\n","\n","n_calls = 0   #nº of consecutive calls to API\n","\n","for race in range(len(race_folders)-1):\n","  # Open each race folder and list local image files\n","  folder_name = os.path.join(beauty_folder, race_folders[race+1])\n","  img_files = os.listdir(folder_name)\n","  \n","  n_imgs = len(img_files) #nº images in the folder\n","  \n","  for img in range(n_imgs):\n","    #Open images one by one\n","    local_image_path = os.path.join (folder_name, img_files[img])\n","    local_image = open(local_image_path, \"rb\")\n","    \n","    #Control nº of calls/min\n","    if n_calls >= 20 :\n","      n_calls = 0\n","      print(\"===== Wait for 1 min =====\")\n","      time.sleep(60) #wait for 1 minute\n","      \n","    n_calls += 1\n","\n","    # Call API\n","    print(\"===== Tag an Image - local =====\")\n","    tags_result_local = computervision_client.tag_image_in_stream(local_image)\n","    \n","    #Store tags set\n","    tag_results.append(tags_result_local.tags)\n","'''\n","END - Tag an Image - local\n","'''"],"metadata":{"id":"o1JgJs56xDq6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Save the results in a dictionary**\n","Once the tags have been obtained for each single image, the next thing to do is to store them in the form of a dictionary. This dictionary has the following keys: '#ID' and 'tags'. \n","\n","The values stored for each key are the different images that have been used to call the API and the obtained set of tags for each of these images, in the form: *tag_name: confidence*, all of them in a matching order.\n","\n","This process need to be repeated both for the original images folder and for the beautified images folder, obtaining two different dictionaries."],"metadata":{"id":"TIBFoFg9-QML"}},{"cell_type":"code","source":["#Create dictionary\n","tags_dict_all = {'#ID':[ ],\n","              'tags':[ ]}\n","              \n","for race in range(len(race_folders)-1):\n","  # Open each race folder and list local image files\n","  folder_name = os.path.join(beauty_folder, race_folders[race+1])\n","  img_files = os.listdir(folder_name)\n","  \n","  n_imgs = len(img_files)\n","  \n","  for img in range(n_imgs):\n","    #Open & store images one by one\n","    tags_dict_all['#ID'].append(img_files[img])\n","    tags_dict_all['tags'].append({})\n","    #Save tags for each image (tag:confidence)\n","    for tag in tag_results[(race*n_imgs)+img]:\n","      tags_dict_all['tags'][(race*n_imgs)+img][tag.name] = tag.confidence*100\n"],"metadata":{"id":"23rW7sw6P4Na"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Write the tags dictionary to a JSON file**\n","To avoid losing the obtained results, the created dictionary is saved in a JSON file.\n","\n","This file is also useful to work with dataframes in order to analyse the tags that are obtained.\n","\n","This process is also repeated for both the original images folder and the beautified images folder, so two different JSON files are obtained at the end, with names \"*beauty_tags.json*\" and \"*original_tags.json*\"."],"metadata":{"id":"R9pjKTcuCsKg"}},{"cell_type":"code","source":["# Write tags to a JSON file\n","with open(\"beauty_tags.json\", \"w\") as outfile:\n","    json.dump(tags_dict_all, outfile)"],"metadata":{"id":"BdavncY5VqG8","colab":{"base_uri":"https://localhost:8080/","height":200},"executionInfo":{"status":"error","timestamp":1674635986048,"user_tz":-60,"elapsed":4,"user":{"displayName":"Núria Camps","userId":"06463818817438967590"}},"outputId":"3c111e60-4fc8-4b70-dc7c-c035b81e3ce8"},"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-da0eccec8ceb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Write tags to a JSON file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"beauty_tags.json\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0moutfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtags_dict_all\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'json' is not defined"]}]},{"cell_type":"markdown","source":["## **Convert local JSON files into Pandas DataFrames**\n","We read the local JSON files previously generated via Pandas, using the *read_json()* method.\n","\n","This method is used to extract the data from JSON files and store them as DataFrame.\n","\n","Doing so, two dataframes are generated: one for the beautified images and the other one for the original ones.\n","\n","These dataframes consist of two columns, the first one for all the analysed images and the second one for the corresponding set of tags obtained for each of these images."],"metadata":{"id":"I0yqBmvqFPVH"}},{"cell_type":"code","source":["#1st dataframe version\n","\n","#Dataframe for the beautified images\n","df1beauty = pd.read_json('beauty_tags.json')\n","\n","#Dataframe for the original images\n","df1original = pd.read_json('original_tags.json')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":423},"id":"VnKM_bzF6mrh","executionInfo":{"status":"ok","timestamp":1674547025716,"user_tz":-60,"elapsed":236,"user":{"displayName":"Núria Camps","userId":"06463818817438967590"}},"outputId":"8a4877a0-a7a7-4fa0-ca18-fdf72b389a85"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["              #ID                                               tags\n","0     7733_01.png  {'person': 98.46811294555664, 'human face': 98...\n","1     1705_01.png  {'human face': 99.54532384872437, 'person': 99...\n","2     3547_01.png  {'human face': 99.87502098083496, 'person': 99...\n","3      396_01.png  {'person': 99.15273785591125, 'human face': 98...\n","4    10834_01.png  {'human face': 99.50470328330994, 'forehead': ...\n","..            ...                                                ...\n","296   7385_01.png  {'human face': 98.80687594413757, 'person': 98...\n","297   2600_01.png  {'human face': 99.3657112121582, 'fashion acce...\n","298   9348_01.png  {'human face': 99.90034103393555, 'person': 99...\n","299   8793_01.png  {'person': 98.4387993812561, 'human face': 97....\n","300   1717_01.png  {'human face': 99.20415878295898, 'person': 97...\n","\n","[301 rows x 2 columns]"],"text/html":["\n","  <div id=\"df-f207b418-b4ca-445e-90a8-a3f6ce355ae5\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>#ID</th>\n","      <th>tags</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>7733_01.png</td>\n","      <td>{'person': 98.46811294555664, 'human face': 98...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1705_01.png</td>\n","      <td>{'human face': 99.54532384872437, 'person': 99...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3547_01.png</td>\n","      <td>{'human face': 99.87502098083496, 'person': 99...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>396_01.png</td>\n","      <td>{'person': 99.15273785591125, 'human face': 98...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>10834_01.png</td>\n","      <td>{'human face': 99.50470328330994, 'forehead': ...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>296</th>\n","      <td>7385_01.png</td>\n","      <td>{'human face': 98.80687594413757, 'person': 98...</td>\n","    </tr>\n","    <tr>\n","      <th>297</th>\n","      <td>2600_01.png</td>\n","      <td>{'human face': 99.3657112121582, 'fashion acce...</td>\n","    </tr>\n","    <tr>\n","      <th>298</th>\n","      <td>9348_01.png</td>\n","      <td>{'human face': 99.90034103393555, 'person': 99...</td>\n","    </tr>\n","    <tr>\n","      <th>299</th>\n","      <td>8793_01.png</td>\n","      <td>{'person': 98.4387993812561, 'human face': 97....</td>\n","    </tr>\n","    <tr>\n","      <th>300</th>\n","      <td>1717_01.png</td>\n","      <td>{'human face': 99.20415878295898, 'person': 97...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>301 rows × 2 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f207b418-b4ca-445e-90a8-a3f6ce355ae5')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-f207b418-b4ca-445e-90a8-a3f6ce355ae5 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-f207b418-b4ca-445e-90a8-a3f6ce355ae5');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":15}]},{"cell_type":"markdown","source":["## Format DataFrame structure\n","In the previous DataFrames, it can be observed that the nested list of tags for each image is put up into a single column 'tags'. Here we are going to flatten the nested list of tags.\n","\n","We now load data using Python json module and after that, json_normalize function is called with the argument *record_path* set to ['tags'], to flatten the nested list in tags.\n","To flatten this nested list, we use the Pandas json_normalize() function\n","\n","Here, the JSON files are loaded using json.loads() function and then the JSON object is passed to json_normalize().\n","\n","For the result to include the images filenames, they are collected throughout the folder (they are the same and in the same order for the beautified and the original ones) and stored in the *img_files* list.\n","Finally, this list is assigned to the Dataframes indices values."],"metadata":{"id":"0-J_SUVTI0XS"}},{"cell_type":"code","source":["#2nd dataframe version\n","\n","#Dataframe for the BEAUTIFIED images\n","df2beauty = json.loads(open('beauty_tags.json').read())\n","\n","#Flatten nested list in tags\n","df2beauty = pd.json_normalize(df2beauty, record_path =['tags'])\n","\n","#Confidence=0 for tags not being assigned to specific images\n","df2beauty[df2beauty.isnull()] = 0\n","\n","#Collect image filenames\n","img_index = []\n","for race in range(len(race_folders)-1):\n","  folder_name = os.path.join(beauty_folder, race_folders[race])\n","  img_files = os.listdir(folder_name)\n","  for img in img_files:\n","    img_index.append(img)\n","\n","#Assign image filenames to DataFrame indices\n","df2beauty.index = img_index\n","\n","\n","#Dataframe for the ORIGINAL images\n","df2original = json.loads(open('original_tags.json').read())\n","\n","#Flatten nested list in tags\n","df2original = pd.json_normalize(df2original, record_path =['tags'])\n","\n","#Confidence=0 for tags not being assigned to specific images\n","df2original[df2original.isnull()] = 0\n","\n","#Assign image filenames to DataFrame indices\n","df2original.index = img_index"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":505},"id":"q6ErmfNt7HPg","executionInfo":{"status":"ok","timestamp":1674547338944,"user_tz":-60,"elapsed":287,"user":{"displayName":"Núria Camps","userId":"06463818817438967590"}},"outputId":"6bb7ab81-97ea-4d2e-fae4-7c7642754150"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                 person  human face    eyebrow        lip       skin  \\\n","5797_01.png   98.468113   98.408616  94.973540  94.833589  92.442918   \n","10396_01.png  99.086314   99.545324  97.391045  93.091023  94.030166   \n","6921_01.png   99.473584   99.875021  97.466552  86.560416  90.748370   \n","4251_01.png   99.152738   98.333335  94.530892  90.832853  94.658494   \n","7385_01.png   96.265638   99.504703  96.621042  89.517248  90.185213   \n","...                 ...         ...        ...        ...        ...   \n","3360_01.png   98.469973   98.806876  87.848037  92.030001  89.097011   \n","9798_01.png   93.424058   99.365711   0.000000  93.042856  84.871948   \n","1730_01.png   99.026918   99.900341   0.000000   0.000000   0.000000   \n","8090_01.png   98.438799   97.949147   0.000000  90.487278  88.906950   \n","9995_01.png   97.536445   99.204159  93.552715  95.528716  92.354059   \n","\n","                  cheek      smile  portrait photography       girl  \\\n","5797_01.png   89.450395  88.346857             87.934518  87.507850   \n","10396_01.png  95.854425   0.000000              0.000000   0.000000   \n","6921_01.png   89.743829  97.387266              0.000000   0.000000   \n","4251_01.png   89.918983   0.000000              0.000000  71.804047   \n","7385_01.png   92.662573   0.000000              0.000000   0.000000   \n","...                 ...        ...                   ...        ...   \n","3360_01.png   86.121821  94.727314              0.000000   0.000000   \n","9798_01.png   90.977645   0.000000              0.000000   0.000000   \n","1730_01.png    0.000000  98.833615              0.000000   0.000000   \n","8090_01.png   89.659834  90.398860              0.000000  57.921273   \n","9995_01.png   84.056246  94.384784             85.408211  59.128118   \n","\n","               forehead  ...  mammal  blue  building  crown jewels  temple  \\\n","5797_01.png   87.333477  ...     0.0   0.0       0.0           0.0     0.0   \n","10396_01.png  97.695827  ...     0.0   0.0       0.0           0.0     0.0   \n","6921_01.png   96.050119  ...     0.0   0.0       0.0           0.0     0.0   \n","4251_01.png   86.446774  ...     0.0   0.0       0.0           0.0     0.0   \n","7385_01.png   97.395873  ...     0.0   0.0       0.0           0.0     0.0   \n","...                 ...  ...     ...   ...       ...           ...     ...   \n","3360_01.png    0.000000  ...     0.0   0.0       0.0           0.0     0.0   \n","9798_01.png    0.000000  ...     0.0   0.0       0.0           0.0     0.0   \n","1730_01.png    0.000000  ...     0.0   0.0       0.0           0.0     0.0   \n","8090_01.png    0.000000  ...     0.0   0.0       0.0           0.0     0.0   \n","9995_01.png    0.000000  ...     0.0   0.0       0.0           0.0     0.0   \n","\n","              screen  newborn  beanie  knit cap   earphone  \n","5797_01.png      0.0      0.0     0.0       0.0   0.000000  \n","10396_01.png     0.0      0.0     0.0       0.0   0.000000  \n","6921_01.png      0.0      0.0     0.0       0.0   0.000000  \n","4251_01.png      0.0      0.0     0.0       0.0   0.000000  \n","7385_01.png      0.0      0.0     0.0       0.0   0.000000  \n","...              ...      ...     ...       ...        ...  \n","3360_01.png      0.0      0.0     0.0       0.0   0.000000  \n","9798_01.png      0.0      0.0     0.0       0.0  68.870705  \n","1730_01.png      0.0      0.0     0.0       0.0   0.000000  \n","8090_01.png      0.0      0.0     0.0       0.0   0.000000  \n","9995_01.png      0.0      0.0     0.0       0.0   0.000000  \n","\n","[301 rows x 173 columns]"],"text/html":["\n","  <div id=\"df-38b8fb9a-f299-4dbc-b762-9a9d20f26ed0\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>person</th>\n","      <th>human face</th>\n","      <th>eyebrow</th>\n","      <th>lip</th>\n","      <th>skin</th>\n","      <th>cheek</th>\n","      <th>smile</th>\n","      <th>portrait photography</th>\n","      <th>girl</th>\n","      <th>forehead</th>\n","      <th>...</th>\n","      <th>mammal</th>\n","      <th>blue</th>\n","      <th>building</th>\n","      <th>crown jewels</th>\n","      <th>temple</th>\n","      <th>screen</th>\n","      <th>newborn</th>\n","      <th>beanie</th>\n","      <th>knit cap</th>\n","      <th>earphone</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>5797_01.png</th>\n","      <td>98.468113</td>\n","      <td>98.408616</td>\n","      <td>94.973540</td>\n","      <td>94.833589</td>\n","      <td>92.442918</td>\n","      <td>89.450395</td>\n","      <td>88.346857</td>\n","      <td>87.934518</td>\n","      <td>87.507850</td>\n","      <td>87.333477</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>10396_01.png</th>\n","      <td>99.086314</td>\n","      <td>99.545324</td>\n","      <td>97.391045</td>\n","      <td>93.091023</td>\n","      <td>94.030166</td>\n","      <td>95.854425</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>97.695827</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>6921_01.png</th>\n","      <td>99.473584</td>\n","      <td>99.875021</td>\n","      <td>97.466552</td>\n","      <td>86.560416</td>\n","      <td>90.748370</td>\n","      <td>89.743829</td>\n","      <td>97.387266</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>96.050119</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>4251_01.png</th>\n","      <td>99.152738</td>\n","      <td>98.333335</td>\n","      <td>94.530892</td>\n","      <td>90.832853</td>\n","      <td>94.658494</td>\n","      <td>89.918983</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>71.804047</td>\n","      <td>86.446774</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>7385_01.png</th>\n","      <td>96.265638</td>\n","      <td>99.504703</td>\n","      <td>96.621042</td>\n","      <td>89.517248</td>\n","      <td>90.185213</td>\n","      <td>92.662573</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>97.395873</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>3360_01.png</th>\n","      <td>98.469973</td>\n","      <td>98.806876</td>\n","      <td>87.848037</td>\n","      <td>92.030001</td>\n","      <td>89.097011</td>\n","      <td>86.121821</td>\n","      <td>94.727314</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>9798_01.png</th>\n","      <td>93.424058</td>\n","      <td>99.365711</td>\n","      <td>0.000000</td>\n","      <td>93.042856</td>\n","      <td>84.871948</td>\n","      <td>90.977645</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>68.870705</td>\n","    </tr>\n","    <tr>\n","      <th>1730_01.png</th>\n","      <td>99.026918</td>\n","      <td>99.900341</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>98.833615</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>8090_01.png</th>\n","      <td>98.438799</td>\n","      <td>97.949147</td>\n","      <td>0.000000</td>\n","      <td>90.487278</td>\n","      <td>88.906950</td>\n","      <td>89.659834</td>\n","      <td>90.398860</td>\n","      <td>0.000000</td>\n","      <td>57.921273</td>\n","      <td>0.000000</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>9995_01.png</th>\n","      <td>97.536445</td>\n","      <td>99.204159</td>\n","      <td>93.552715</td>\n","      <td>95.528716</td>\n","      <td>92.354059</td>\n","      <td>84.056246</td>\n","      <td>94.384784</td>\n","      <td>85.408211</td>\n","      <td>59.128118</td>\n","      <td>0.000000</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>301 rows × 173 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-38b8fb9a-f299-4dbc-b762-9a9d20f26ed0')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-38b8fb9a-f299-4dbc-b762-9a9d20f26ed0 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-38b8fb9a-f299-4dbc-b762-9a9d20f26ed0');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":20}]},{"cell_type":"markdown","source":["# **Some statistics of the Microsoft Azure ITA**\n","Hereunder, the analysis of the tags obtained with the Image Tagging Algorithm by Microsoft Azure is carried out.\n","\n","## **Unique tags count**\n","The number of unique tags obtained in the case of original images and in the case of beautified images are counted, by measuring the number of columns each DataFrame has.\n","\n","The number of unique tags assigned by the Azure service to original faces and their beautified versions are the following:\n","* **200 unique tags** are assigned to **original** faces.\n","* **173 unique tags** are assigned to **beautified** faces.\n","\n","Note that the number of unique tags decreases after beautification."],"metadata":{"id":"cODQaLcxPYOK"}},{"cell_type":"code","source":["#Collect tags assigned to original & beautified faces\n","original_tags = df2original.columns.values\n","beauty_tags = df2beauty.columns.values\n","\n","#nº unique tags in original & beautified faces\n","N_original = len(original_tags)\n","N_beauty = len(beauty_tags)"],"metadata":{"id":"m49tUKV6dj9u"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Lost and new tags count\n","Here, the number of original tags lost (only present in original case) and the number of new tags (only present after beautification) are counted.\n","\n","For that, we go through the tags in the original set one by one and check if they are in the beautified tag set. If so, the repeated tags and its corresponding index are stored. Otherwise, the tag is stored in the *lost_tags* list.\n","\n","The new tags are those tags in the beauty set that are not in the indices corresponding to the repeated tags that are stored previously.\n","\n","Measuring the lenght of the *new_tags* and the *lost_tags* lists, the obtained numbers are the following:\n","* Among the original 200 tags, **47** (more than 23%) are **lost** (not present after beautification)\n","* On the other hand, **20** tags are **new** (only present after beautification"],"metadata":{"id":"BHoDttN_STTu"}},{"cell_type":"code","source":["#Count repeated, lost and new tags\n","N_repeated = 0\n","repeated_tags = []\n","lost_tags = []\n","\n","#Indices of the repeated tags\n","rep_index = []\n","\n","#Go through original tags one by one\n","for t in range(len(original_tags)):\n","  #Check if original tag is in beauty set\n","  a = np.where(beauty_tags == original_tags[t])\n","  if (np.size(a) > 0):\n","    #Repeated tag: save tag & index\n","    rep_index.append(a[0][0])\n","    repeated_tags.append(original_tags[t])\n","  else:\n","    #Lost tag\n","    lost_tags.append(original_tags[t])\n","\n","#New tags: beauty tags that aren't repeated\n","new_tags = np.delete(beauty_tags, rep_index)\n","\n","#Repeated tags in original & beauty\n","N_repeated = len(repeated_tags)\n","\n","#Original tags lost\n","N_lost = len(lost_tags)\n","\n","#New beauty tags\n","N_new = len(new_tags)"],"metadata":{"id":"NnET0Q6XR5MY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Attractiveness rating\n","Here, an study on how beauty filters are changing the algorithm's rating of attractiveness is carried out. In this sense, the number of faces that are considered attractive only after beautification and only before beautification are counted.\n","\n","To do so, a set of tags to refer to attractiveness is defined among all the tags obtained with the Azure algorithm, namely \"*child model, cool, gentleman, dating, posing, love, kiss, romance, makeover*\", so we consider attractive the images that are assigned, at least, one of these tags.\n","\n","Then, going through all the image filenames one by one (that are the same for the original face and its corresponding beautified version) it is checked for each image if any of the tags associated to attractiveness has been assigned to its original version or its beautified one. If so, the index of the corresponding image is stored to use it later.\n","\n","Once these checks have been carried out, for each of the original and beautified images, the non-attractive faces are the images that are not in the stored indices corresponding to attractive faces, whereas attractive faces are those that correspond, in fact, to these indices.\n","\n","To obtain the faces that are considered attractive only after beautification, the intersection among the images that are in the original non-attractive subset and in the beautified attractive subset is obtained.\n","\n","On the other hand, to obtain the faces that are considered attractive only before beautification, the intersection among the images that are in the original attractive subset and in the beautified non-attractive subset is performed.\n","\n","Measuring the lenght of the intersected lists, the resulting numbers are the following:\n","* The faces that are considered **attractive** by the algorithm **only after beautification** are **16**, which is **5,32%** of images.\n","* Conversely, the faces that are considered **attractive only before beautification** are **15**, which is **4,98%** of images."],"metadata":{"id":"NK6jAsSCWa3-"}},{"cell_type":"code","source":["#Set of tags referring to attractiveness\n","att_set = ['child model', 'cool', 'gentleman', 'dating', 'posing', 'love', 'kiss', 'romance', 'makeover'] \n","\n","att_beauty = []\n","att_original = []\n","\n","#Check if attractiveness tags are in original & beautified sets\n","for tag in att_set:\n","  b = np.where(df2beauty.columns.values == tag) #ver si beauty contiene esa tag\n","  c = np.where(df2original.columns.values == tag) #ver si original contiene esa tag\n","  if np.size(b)>0:\n","    att_beauty.append(tag)\n","  if np.size(c)>0:\n","    att_original.append(tag)\n","\n","#Attractiveness subset\n","beauty = df2beauty.loc[:, att_beauty]\n","original = df2original.loc[:, att_original]\n","\n","beauty_index = []\n","original_index = []\n","\n","for img in range(len(img_index)):\n","  #Check if any attractiveness tag is assigned\n","  d = np.where(beauty.loc[img_index[img],:]>0)\n","  e = np.where(original.loc[img_index[img],:]>0)\n","  #Collect indexes of attractive images\n","  if (np.size(d)>0):\n","    beauty_index.append(img)\n","  if (np.size(e)>0):\n","    original_index.append(img)\n","\n","#Non attractive: images that aren't in attractive indices\n","non_attractive_original = np.delete(img_index, original_index)\n","non_attractive_beauty = np.delete(img_index, beauty_index)\n","\n","#Attractive: images that are in attractive indices\n","attractive_original = [img_index[i] for i in original_index]\n","attractive_beauty = [img_index[i] for i in beauty_index]\n","\n","\n","#Attractive only after beautification\n","only_beauty = np.intersect1d(non_attractive_original, attractive_beauty)\n","N_only_after= len(only_beauty)\n","\n","#Attractive only before beautification\n","only_original = np.intersect1d(non_attractive_beauty, attractive_original)\n","N_only_before = len(only_original)"],"metadata":{"id":"Xy7pjtsjdxiQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Statistics summary\n","# % of lost tags from original case when beautified\n","perc_lost = (N_lost/N_original)*100\n","\n","# % of faces that are considered attractive only after beautification\n","perc_after = (N_only_after/len(img_index))*100\n","\n","# % of faces that are considered attractive only before beautification\n","perc_before = (N_only_before/len(img_index))*100"],"metadata":{"id":"B8D_i1izopuq"},"execution_count":null,"outputs":[]}]}